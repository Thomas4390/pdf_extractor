# SpÃ©cifications Techniques - Vision LLM PDF Extractor v2.0

## Vue d'ensemble

Migration complÃ¨te du pipeline d'extraction PDF vers une architecture unifiÃ©e dans `src/`. L'objectif est de consolider :
- L'extraction VLM (dÃ©jÃ  implÃ©mentÃ©e)
- L'unification des donnÃ©es (depuis `scripts/unify_notation.py`)
- Le client Monday.com (depuis `scripts/monday_automation.py`)
- L'application Streamlit (depuis `scripts/app.py`)

---

## Architecture Cible

### Structure du rÃ©pertoire `src/`

```
src/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ pipeline.py                    # Orchestrateur principal
â”‚
â”œâ”€â”€ extractors/                    # Extraction VLM (existant)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                    # BaseExtractor[T]
â”‚   â”œâ”€â”€ uv_extractor.py
â”‚   â”œâ”€â”€ idc_extractor.py
â”‚   â”œâ”€â”€ idc_statement_extractor.py
â”‚   â””â”€â”€ assomption_extractor.py
â”‚
â”œâ”€â”€ models/                        # SchÃ©mas Pydantic (existant)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ uv.py
â”‚   â”œâ”€â”€ idc.py
â”‚   â”œâ”€â”€ idc_statement.py
â”‚   â”œâ”€â”€ assomption.py
â”‚   â””â”€â”€ common.py
â”‚
â”œâ”€â”€ clients/                       # Clients API
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openrouter.py              # Client VLM (existant)
â”‚   â”œâ”€â”€ cache.py                   # Cache local (existant)
â”‚   â”œâ”€â”€ monday.py                  # Client Monday.com (NOUVEAU)
â”‚   â”œâ”€â”€ json_repair.py             # (existant)
â”‚   â””â”€â”€ retry_handler.py           # (existant)
â”‚
â”œâ”€â”€ utils/                         # Utilitaires
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                  # Configuration (existant)
â”‚   â”œâ”€â”€ pdf.py                     # PDF utils (existant)
â”‚   â”œâ”€â”€ model_registry.py          # Registry (existant)
â”‚   â”œâ”€â”€ advisor_matcher.py         # Matching conseillers (existant)
â”‚   â””â”€â”€ data_unifier.py            # Unification donnÃ©es (NOUVEAU)
â”‚
â”œâ”€â”€ prompts/                       # Prompts YAML (existant)
â”‚   â”œâ”€â”€ uv.yaml
â”‚   â”œâ”€â”€ idc.yaml
â”‚   â”œâ”€â”€ idc_statement.yaml
â”‚   â””â”€â”€ assomption.yaml
â”‚
â”œâ”€â”€ app/                           # Application Streamlit (NOUVEAU)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Point d'entrÃ©e Streamlit
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ upload.py              # Page 1: Upload & Config
â”‚   â”‚   â”œâ”€â”€ preview.py             # Page 2: Preview & Edit
â”‚   â”‚   â””â”€â”€ export.py              # Page 3: Export Monday.com
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_editor.py         # Ã‰diteur de donnÃ©es
â”‚   â”‚   â”œâ”€â”€ advisor_manager.py     # Gestion conseillers
â”‚   â”‚   â””â”€â”€ progress.py            # Indicateurs de progression
â”‚   â””â”€â”€ state.py                   # Gestion Ã©tat Streamlit
â”‚
â””â”€â”€ tests/                         # Tests (existant)
    â”œâ”€â”€ test_uv.py
    â”œâ”€â”€ test_idc.py
    â”œâ”€â”€ test_assomption.py
    â””â”€â”€ test_idc_statement.py
```

---

## User Stories

### US-1: Extraction VLM (existant - validÃ©)
**En tant qu'** utilisateur
**Je veux** extraire les donnÃ©es des rapports PDF via un modÃ¨le de vision
**Afin d'** obtenir une extraction fiable et maintenable

**CritÃ¨res d'acceptation:**
- [x] Conversion PDF â†’ images PNG Ã  300 DPI via PyMuPDF
- [x] Envoi des pages sÃ©lectionnÃ©es au VLM
- [x] Validation Pydantic du rÃ©sultat JSON
- [x] Retry automatique avec fallback model
- [x] Cache local par hash SHA-256

### US-2: Unification des donnÃ©es (NOUVEAU)
**En tant qu'** utilisateur
**Je veux** que les donnÃ©es extraites soient standardisÃ©es automatiquement
**Afin d'** avoir un format cohÃ©rent pour Monday.com

**CritÃ¨res d'acceptation:**
- [ ] Classe `DataUnifier` convertissant les modÃ¨les Pydantic en DataFrame
- [ ] Mapping automatique vers les colonnes franÃ§aises
- [ ] DÃ©tection du type de board basÃ©e sur la source PDF
- [ ] Calcul de commission uniforme : `prime Ã— taux_partage Ã— taux_commission`
- [ ] Normalisation des noms de conseillers via AdvisorMatcher

### US-3: Client Monday.com (NOUVEAU)
**En tant qu'** utilisateur
**Je veux** uploader les donnÃ©es vers Monday.com
**Afin d'** intÃ©grer les commissions dans mon workflow

**CritÃ¨res d'acceptation:**
- [ ] CrÃ©ation automatique des colonnes manquantes
- [ ] Upload batch avec limite de parallÃ©lisme
- [ ] Gestion des types de colonnes (numbers, text, status, date)
- [ ] Support des deux types de boards (Historical/Sales)

### US-4: Pipeline orchestrÃ© (NOUVEAU)
**En tant qu'** utilisateur
**Je veux** un pipeline unifiÃ© extraction â†’ unification â†’ upload
**Afin d'** automatiser le traitement complet

**CritÃ¨res d'acceptation:**
- [ ] Classe `Pipeline` orchestrant les 3 Ã©tapes
- [ ] Traitement batch parallÃ¨le (max 3 PDFs simultanÃ©s)
- [ ] Gestion des erreurs : donnÃ©es partielles acceptÃ©es + warning
- [ ] Logs dÃ©taillÃ©s de progression

### US-5: Application Streamlit (REFACTOR)
**En tant qu'** utilisateur
**Je veux** une interface pour gÃ©rer l'extraction et l'upload
**Afin d'** avoir un contrÃ´le visuel sur le processus

**CritÃ¨res d'acceptation:**
- [ ] Upload batch de PDFs (drag & drop multiple)
- [ ] PrÃ©visualisation des donnÃ©es extraites
- [ ] Ã‰dition manuelle avant upload
- [ ] Gestion des conseillers (ajout/modification)
- [ ] Indicateurs de progression
- [ ] Workflow en 3 Ã©tapes : Upload â†’ Preview â†’ Export

---

## DÃ©tails Techniques d'ImplÃ©mentation

### 1. DataUnifier (`src/utils/data_unifier.py`)

```python
from enum import Enum
from typing import Union
import pandas as pd

from ..models import UVReport, IDCReport, IDCStatementReport, AssomptionReport


class BoardType(Enum):
    """Type de board Monday.com."""
    HISTORICAL_PAYMENTS = "HISTORICAL_PAYMENTS"  # IDC_STATEMENT
    SALES_PRODUCTION = "SALES_PRODUCTION"        # UV, IDC, ASSOMPTION


class DataUnifier:
    """
    Convertit les modÃ¨les Pydantic extraits en DataFrames standardisÃ©s.

    ResponsabilitÃ©s:
    - Conversion des modÃ¨les Pydantic vers DataFrame pandas
    - Mapping vers les colonnes franÃ§aises finales
    - Calcul des commissions
    - Normalisation des noms de conseillers
    """

    # Colonnes finales pour Paiements Historiques (13 colonnes)
    FINAL_COLUMNS_HISTORICAL = [
        '# de Police', 'Nom Client', 'Compagnie', 'Statut',
        'Conseiller', 'VerifiÃ©', 'PA', 'Com', 'Boni',
        'Sur-Com', 'ReÃ§u', 'Date', 'Texte'
    ]

    # Colonnes finales pour Ventes et Production (19 colonnes)
    FINAL_COLUMNS_SALES = [
        'Date', '# de Police', 'Nom Client', 'Compagnie', 'Statut',
        'Conseiller', 'Complet', 'PA', 'Lead/MC', 'Com', 'ReÃ§u 1',
        'Boni', 'ReÃ§u 2', 'Sur-Com', 'ReÃ§u 3', 'Total',
        'Total ReÃ§u', 'Paie', 'Texte'
    ]

    # Mapping source â†’ type de board
    SOURCE_TO_BOARD_TYPE = {
        'UV': BoardType.SALES_PRODUCTION,
        'IDC': BoardType.SALES_PRODUCTION,
        'ASSOMPTION': BoardType.SALES_PRODUCTION,
        'IDC_STATEMENT': BoardType.HISTORICAL_PAYMENTS,
    }

    def __init__(self, advisor_matcher=None):
        self.advisor_matcher = advisor_matcher

    def unify(
        self,
        report: Union[UVReport, IDCReport, IDCStatementReport, AssomptionReport],
        source: str
    ) -> tuple[pd.DataFrame, BoardType]:
        """
        Convertit un rapport en DataFrame standardisÃ©.

        Args:
            report: ModÃ¨le Pydantic extrait
            source: Type de source ('UV', 'IDC', 'IDC_STATEMENT', 'ASSOMPTION')

        Returns:
            Tuple (DataFrame avec colonnes franÃ§aises, BoardType)
        """
        board_type = self.SOURCE_TO_BOARD_TYPE[source]

        # Conversion spÃ©cifique par source
        if source == 'UV':
            df = self._convert_uv(report)
        elif source == 'IDC':
            df = self._convert_idc(report)
        elif source == 'IDC_STATEMENT':
            df = self._convert_idc_statement(report)
        elif source == 'ASSOMPTION':
            df = self._convert_assomption(report)
        else:
            raise ValueError(f"Source inconnue: {source}")

        # Appliquer le schÃ©ma de colonnes final
        df = self._apply_final_schema(df, board_type)

        # Normaliser les noms de conseillers
        if self.advisor_matcher and 'Conseiller' in df.columns:
            df['Conseiller'] = df['Conseiller'].apply(
                lambda x: self.advisor_matcher.match(x) if pd.notna(x) else x
            )

        return df, board_type

    def _calculate_commission(
        self,
        premium: float,
        sharing_rate: float,
        commission_rate: float
    ) -> float:
        """
        Calcule la commission selon la formule universelle.

        commission = prime Ã— taux_partage Ã— taux_commission
        """
        return premium * (sharing_rate / 100) * (commission_rate / 100)

    def _convert_uv(self, report: UVReport) -> pd.DataFrame:
        """Convertit un rapport UV en DataFrame standardisÃ©."""
        # ... implÃ©mentation

    def _convert_idc(self, report: IDCReport) -> pd.DataFrame:
        """Convertit un rapport IDC en DataFrame standardisÃ©."""
        # ... implÃ©mentation

    def _convert_idc_statement(self, report: IDCStatementReport) -> pd.DataFrame:
        """Convertit un relevÃ© IDC en DataFrame standardisÃ©."""
        # ... implÃ©mentation

    def _convert_assomption(self, report: AssomptionReport) -> pd.DataFrame:
        """Convertit un rapport Assomption en DataFrame standardisÃ©."""
        # ... implÃ©mentation

    def _apply_final_schema(self, df: pd.DataFrame, board_type: BoardType) -> pd.DataFrame:
        """Applique le schÃ©ma de colonnes final selon le type de board."""
        if board_type == BoardType.HISTORICAL_PAYMENTS:
            columns = self.FINAL_COLUMNS_HISTORICAL
        else:
            columns = self.FINAL_COLUMNS_SALES

        # Ajouter les colonnes manquantes avec valeurs par dÃ©faut
        for col in columns:
            if col not in df.columns:
                df[col] = None

        return df[columns]
```

### 2. Client Monday.com (`src/clients/monday.py`)

```python
import httpx
from typing import Optional
import pandas as pd

from ..utils.data_unifier import BoardType


class MondayClient:
    """
    Client pour l'API Monday.com avec support GraphQL.

    FonctionnalitÃ©s:
    - CRUD sur les boards/items
    - CrÃ©ation automatique des colonnes manquantes
    - Upload batch avec rate limiting
    """

    BASE_URL = "https://api.monday.com/v2"

    # Mapping colonnes â†’ types Monday.com
    COLUMN_TYPES = {
        '# de Police': 'text',
        'Nom Client': 'text',
        'Compagnie': 'text',
        'Statut': 'status',
        'Conseiller': 'text',
        'VerifiÃ©': 'checkbox',
        'PA': 'numbers',
        'Com': 'numbers',
        'Boni': 'numbers',
        'Sur-Com': 'numbers',
        'ReÃ§u': 'numbers',
        'ReÃ§u 1': 'numbers',
        'ReÃ§u 2': 'numbers',
        'ReÃ§u 3': 'numbers',
        'Total': 'numbers',
        'Total ReÃ§u': 'numbers',
        'Date': 'date',
        'Paie': 'date',
        'Texte': 'long_text',
        'Complet': 'checkbox',
        'Lead/MC': 'text',
    }

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.headers = {
            "Authorization": api_key,
            "Content-Type": "application/json",
            "API-Version": "2024-01"
        }

    async def upload_dataframe(
        self,
        df: pd.DataFrame,
        board_id: str,
        group_id: Optional[str] = None,
        create_missing_columns: bool = True
    ) -> dict:
        """
        Upload un DataFrame vers Monday.com.

        Args:
            df: DataFrame avec colonnes franÃ§aises
            board_id: ID du board cible
            group_id: ID du groupe (optionnel)
            create_missing_columns: CrÃ©er les colonnes manquantes

        Returns:
            RÃ©sultat de l'upload avec statistiques
        """
        # 1. RÃ©cupÃ©rer les colonnes existantes
        existing_columns = await self.get_columns(board_id)

        # 2. CrÃ©er les colonnes manquantes si autorisÃ©
        if create_missing_columns:
            for col in df.columns:
                if col not in existing_columns and col != 'Nom Client':
                    col_type = self.COLUMN_TYPES.get(col, 'text')
                    await self.create_column(board_id, col, col_type)

        # 3. Upload les items en batch
        results = await self._batch_upload(df, board_id, group_id)

        return results

    async def get_columns(self, board_id: str) -> dict:
        """RÃ©cupÃ¨re les colonnes d'un board."""
        query = """
        query ($boardId: [ID!]) {
            boards(ids: $boardId) {
                columns {
                    id
                    title
                    type
                }
            }
        }
        """
        # ... implÃ©mentation

    async def create_column(
        self,
        board_id: str,
        title: str,
        column_type: str
    ) -> str:
        """CrÃ©e une colonne sur le board."""
        # ... implÃ©mentation

    async def _batch_upload(
        self,
        df: pd.DataFrame,
        board_id: str,
        group_id: Optional[str],
        batch_size: int = 50
    ) -> dict:
        """Upload les items par batch."""
        # ... implÃ©mentation
```

### 3. Pipeline (`src/pipeline.py`)

```python
import asyncio
from pathlib import Path
from typing import Optional
from dataclasses import dataclass

from .extractors import (
    UVExtractor, IDCExtractor,
    IDCStatementExtractor, AssomptionExtractor
)
from .utils.data_unifier import DataUnifier, BoardType
from .utils.advisor_matcher import AdvisorMatcher
from .clients.monday import MondayClient


@dataclass
class PipelineResult:
    """RÃ©sultat du pipeline pour un PDF."""
    pdf_path: str
    source: str
    board_type: BoardType
    dataframe: 'pd.DataFrame'
    success: bool
    warnings: list[str]
    error: Optional[str] = None


class Pipeline:
    """
    Orchestrateur du pipeline complet:
    PDF â†’ Extraction VLM â†’ Unification â†’ Upload Monday.com
    """

    # Mapping extension/pattern â†’ source
    SOURCE_DETECTION = {
        'uv': 'UV',
        'idc_statement': 'IDC_STATEMENT',
        'idc': 'IDC',
        'assomption': 'ASSOMPTION',
    }

    def __init__(
        self,
        monday_api_key: Optional[str] = None,
        max_parallel: int = 3
    ):
        # Extracteurs
        self.extractors = {
            'UV': UVExtractor(),
            'IDC': IDCExtractor(),
            'IDC_STATEMENT': IDCStatementExtractor(),
            'ASSOMPTION': AssomptionExtractor(),
        }

        # Unificateur avec advisor matcher
        self.advisor_matcher = AdvisorMatcher()
        self.unifier = DataUnifier(advisor_matcher=self.advisor_matcher)

        # Client Monday (optionnel)
        self.monday_client = MondayClient(monday_api_key) if monday_api_key else None

        # ContrÃ´le de parallÃ©lisme
        self.semaphore = asyncio.Semaphore(max_parallel)

    async def process_pdf(
        self,
        pdf_path: str | Path,
        source: Optional[str] = None
    ) -> PipelineResult:
        """
        Traite un PDF unique.

        Args:
            pdf_path: Chemin vers le PDF
            source: Type de source (auto-dÃ©tectÃ© si None)

        Returns:
            PipelineResult avec DataFrame et mÃ©tadonnÃ©es
        """
        pdf_path = Path(pdf_path)
        warnings = []

        # Auto-dÃ©tection de la source
        if source is None:
            source = self._detect_source(pdf_path)

        try:
            async with self.semaphore:
                # 1. Extraction VLM
                extractor = self.extractors[source]
                report = await extractor.extract(pdf_path)

                # 2. Unification
                df, board_type = self.unifier.unify(report, source)

                # VÃ©rifier si donnÃ©es partielles
                if len(df) == 0:
                    warnings.append("Aucune donnÃ©e extraite")

                return PipelineResult(
                    pdf_path=str(pdf_path),
                    source=source,
                    board_type=board_type,
                    dataframe=df,
                    success=True,
                    warnings=warnings
                )

        except Exception as e:
            return PipelineResult(
                pdf_path=str(pdf_path),
                source=source,
                board_type=BoardType.SALES_PRODUCTION,
                dataframe=pd.DataFrame(),
                success=False,
                warnings=warnings,
                error=str(e)
            )

    async def process_batch(
        self,
        pdf_paths: list[str | Path],
        source: Optional[str] = None
    ) -> list[PipelineResult]:
        """
        Traite plusieurs PDFs en parallÃ¨le (max 3 simultanÃ©s).

        Args:
            pdf_paths: Liste des chemins PDF
            source: Type de source commun (auto-dÃ©tectÃ© si None)

        Returns:
            Liste des rÃ©sultats
        """
        tasks = [
            self.process_pdf(path, source)
            for path in pdf_paths
        ]
        return await asyncio.gather(*tasks)

    async def upload_to_monday(
        self,
        result: PipelineResult,
        board_id: str,
        group_id: Optional[str] = None
    ) -> dict:
        """
        Upload les rÃ©sultats vers Monday.com.

        Args:
            result: RÃ©sultat du pipeline
            board_id: ID du board cible
            group_id: ID du groupe (optionnel)

        Returns:
            Statistiques d'upload
        """
        if not self.monday_client:
            raise ValueError("Client Monday.com non configurÃ©")

        return await self.monday_client.upload_dataframe(
            result.dataframe,
            board_id,
            group_id
        )

    def _detect_source(self, pdf_path: Path) -> str:
        """DÃ©tecte le type de source depuis le chemin/nom du fichier."""
        path_str = str(pdf_path).lower()

        for pattern, source in self.SOURCE_DETECTION.items():
            if pattern in path_str:
                return source

        # Fallback: demander Ã  l'utilisateur ou lever une erreur
        raise ValueError(f"Impossible de dÃ©tecter la source pour: {pdf_path}")
```

### 4. Application Streamlit (`src/app/main.py`)

```python
import streamlit as st
from pathlib import Path

from .state import init_session_state
from .pages import upload, preview, export


def main():
    st.set_page_config(
        page_title="Insurance Commission Extractor",
        page_icon="ğŸ“Š",
        layout="wide"
    )

    # Initialiser l'Ã©tat de session
    init_session_state()

    # Navigation par Ã©tapes
    steps = ["1. Upload", "2. Preview", "3. Export"]
    current_step = st.session_state.get('current_step', 0)

    # Afficher les onglets de navigation
    cols = st.columns(len(steps))
    for i, (col, step) in enumerate(zip(cols, steps)):
        with col:
            if i < current_step:
                st.success(step + " âœ“")
            elif i == current_step:
                st.info(step + " â†")
            else:
                st.text(step)

    st.divider()

    # Afficher la page correspondante
    if current_step == 0:
        upload.render()
    elif current_step == 1:
        preview.render()
    elif current_step == 2:
        export.render()


if __name__ == "__main__":
    main()
```

---

## SchÃ©mas de donnÃ©es

### Colonnes Historical Payments (13 colonnes)

| Colonne | Type Monday | Description |
|---------|-------------|-------------|
| # de Police | text | NumÃ©ro de contrat |
| Nom Client | text | Nom de l'assurÃ© (item_name) |
| Compagnie | text | Nom de l'assureur |
| Statut | status | Statut du paiement |
| Conseiller | text | Nom normalisÃ© du conseiller |
| VerifiÃ© | checkbox | Validation manuelle |
| PA | numbers | Prime annualisÃ©e ($) |
| Com | numbers | Commission ($) |
| Boni | numbers | Bonus ($) |
| Sur-Com | numbers | Sur-commission ($) |
| ReÃ§u | numbers | Montant reÃ§u ($) |
| Date | date | Date du paiement |
| Texte | long_text | Commentaires |

### Colonnes Sales Production (19 colonnes)

| Colonne | Type Monday | Description |
|---------|-------------|-------------|
| Date | date | Date d'effet |
| # de Police | text | NumÃ©ro de contrat |
| Nom Client | text | Nom de l'assurÃ© (item_name) |
| Compagnie | text | Nom de l'assureur |
| Statut | status | Statut de la vente |
| Conseiller | text | Nom normalisÃ© du conseiller |
| Complet | checkbox | Dossier complet |
| PA | numbers | Prime annualisÃ©e ($) |
| Lead/MC | text | Type de partage |
| Com | numbers | Commission ($) |
| ReÃ§u 1 | numbers | Commission reÃ§ue ($) |
| Boni | numbers | Bonus ($) |
| ReÃ§u 2 | numbers | Bonus reÃ§u ($) |
| Sur-Com | numbers | Sur-commission ($) |
| ReÃ§u 3 | numbers | Sur-commission reÃ§ue ($) |
| Total | numbers | Total commissions ($) |
| Total ReÃ§u | numbers | Total reÃ§u ($) |
| Paie | date | Date de paiement |
| Texte | long_text | Commentaires |

### Mapping Source â†’ Board Type

| Source PDF | Board Type | Raison |
|------------|------------|--------|
| UV | SALES_PRODUCTION | Rapport de ventes |
| IDC | SALES_PRODUCTION | Propositions soumises |
| ASSOMPTION | SALES_PRODUCTION | Rapport de rÃ©munÃ©ration |
| IDC_STATEMENT | HISTORICAL_PAYMENTS | RelevÃ©s de paiements historiques |

---

## Gestion des erreurs

### StratÃ©gie de fallback VLM

```
1. Tentative avec modÃ¨le principal (qwen/qwen2.5-vl-72b-instruct)
   â†“ Ã©chec
2. Retry 1x avec le mÃªme modÃ¨le
   â†“ Ã©chec
3. Fallback vers modÃ¨le secondaire (qwen/qwen3-vl-235b-a22b-instruct)
   â†“ Ã©chec
4. Retry 1x avec modÃ¨le secondaire
   â†“ Ã©chec
5. Retourner donnÃ©es partielles + warning
```

### Comportement en cas de donnÃ©es partielles

- Les donnÃ©es extraites (mÃªme incomplÃ¨tes) sont retournÃ©es dans le DataFrame
- Un warning est ajoutÃ© au `PipelineResult.warnings`
- L'utilisateur voit un indicateur visuel dans Streamlit
- L'upload vers Monday.com reste possible (l'utilisateur peut Ã©diter avant)

---

## Variables d'environnement

```env
# OpenRouter API (Vision LLM)
OPENROUTER_API_KEY=sk-or-v1-xxxxx

# Monday.com API
MONDAY_API_KEY=your_jwt_token_here

# Configuration optionnelle
VLM_MAX_RETRIES=2
VLM_TIMEOUT_SECONDS=120
BATCH_MAX_PARALLEL=3
```

---

## DÃ©pendances

```toml
[project]
dependencies = [
    # Extraction PDF
    "pymupdf>=1.24.0",

    # API clients
    "httpx>=0.27.0",

    # Data processing
    "pandas>=2.0.0",
    "pydantic>=2.6.0",

    # Fuzzy matching
    "rapidfuzz>=3.0.0",

    # Configuration
    "python-dotenv>=1.0.0",
    "pyyaml>=6.0.0",

    # Streamlit
    "streamlit>=1.30.0",

    # JSON repair
    "json-repair>=0.25.0",
]
```

---

## Limitations connues

1. **PDF only**: Import depuis Monday.com non supportÃ© (simplifiÃ© par design)
2. **CoÃ»t API**: ~0.01-0.05$ par page selon le modÃ¨le VLM
3. **Latence**: 5-15 secondes par PDF (extraction VLM)
4. **ParallÃ©lisme**: Maximum 3 PDFs simultanÃ©s pour Ã©viter rate limiting
5. **DonnÃ©es partielles**: AcceptÃ©es avec warning (responsabilitÃ© utilisateur)

---

## Workflow complet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Streamlit                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ 1.Upload â”‚ â†’  â”‚ 2.Previewâ”‚ â†’  â”‚ 3.Export â”‚              â”‚
â”‚  â”‚          â”‚    â”‚ & Edit   â”‚    â”‚ Monday   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚               â”‚               â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pipeline    â”‚ â”‚  DataUnifier  â”‚ â”‚ MondayClient  â”‚
â”‚               â”‚ â”‚               â”‚ â”‚               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ Pydantic â†’   â”‚ â”‚ DataFrame â†’  â”‚
â”‚ â”‚ Extractor â”‚ â”‚ â”‚ DataFrame    â”‚ â”‚ GraphQL      â”‚
â”‚ â”‚ (VLM)     â”‚ â”‚ â”‚ + French colsâ”‚ â”‚ API          â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚               â”‚ â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
   Cache local    Colonnes FR     Board Monday
   (SHA-256)      normalisÃ©es     avec items
```

---

## Prochaines Ã©tapes d'implÃ©mentation

1. **Phase 1**: CrÃ©er `src/utils/data_unifier.py`
   - ImplÃ©menter `DataUnifier` avec les 4 convertisseurs
   - Tests unitaires avec modÃ¨les Pydantic existants

2. **Phase 2**: CrÃ©er `src/clients/monday.py`
   - Migrer depuis `scripts/monday_automation.py`
   - Simplifier (retirer import Monday)
   - Tests d'intÃ©gration

3. **Phase 3**: CrÃ©er `src/pipeline.py`
   - Orchestration des composants
   - Gestion du batch parallÃ¨le
   - Tests end-to-end

4. **Phase 4**: CrÃ©er `src/app/`
   - Structure multi-pages Streamlit
   - Migration UX depuis `scripts/app.py`
   - Tests manuels

---

*Document gÃ©nÃ©rÃ© le 8 janvier 2026*
